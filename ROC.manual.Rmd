---
title: "Prediction manual"
author: "Lasse Ruokolasinen"
date: "1/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
source('roc.curve.R')
source('cv.roc.curve.R')
source('confusion.matrix.R')
```
## Inspect data
I'll use the **GermanCredit** data in the **caret** package as an example here, where the feature 'Class' is used as the variable of interest (label). Here the idea is to predict "bad" customers, so we'll need to swap the class ordering for convenience:
```{r}
require(caret,quietly = T)
data(GermanCredit)
data = GermanCredit
data$Class = factor(data$Class,levels=levels(data$Class)[c(2,1)])
table(data$Class)
```

## Example (1): prediction and ROC-curve for a single split-validation model
The first thing to do is to split the data. In this case 70% of data points are assigned to the trainibng set and the rest are reserved for testing.
```{r}
y = data$Class
train = createDataPartition(y,p = 0.7)$Resample1
head(train)
```
The vector *train* now contains row indices that will be used to subset the data.

Next, let's train a clasification tree model on the training data. Also, we'll evaluate model performance of the training data.
```{r}
require(rpart,quietly = T)
model1 = rpart(Class~.,data=data[train,])
pred1.tr = predict(model1,data[train,],type='class')
conf1.tr = confusion.matrix(pred1.tr,y[train])
```

The object *conf1.tr* now contains several perfomancde metrics, for example the the confusion matrix and classification accuracy:
```{r}
conf1.tr[c(1,4)]
```

Let's now pretend that we are happy with the model and proceed to testing its predictive performance on new data (normally one would, e.g., test several different approaches to find the best model before this next step):
```{r}
pred1.te = predict(model1,data[-train,],type='class')
conf1.te = confusion.matrix(pred1.te,y[-train])
```
```{r}
conf1.te[c(1,4)]
```
It seems that the performance on testing data is on par with that on the training data, indicating that the model is sufficiently general (no overfitting). Next we can look at the ROC-curve, which is another measure of model performance.
```{r,fig.width=5,fig.height=4,message=FALSE}
p = roc.curve(model = model1,test.data = data[-train,],response.variable = 'Class')
```

## Example (2): cross-validated model
Especially with small data sets, one can improve the generality of a model by performing several splits instead of one, which is essentially what happens in cross-validation. Here one creates several splits (folds), each of which is used as the testing set, while the remaing data is used for training. Thus, the proportion of data used for training depends on the number of folds.
```{r}
folds = createFolds(y,k = 5)
head(as.data.frame(folds))
```
Next we need to train a model for each fold. Remember, each fold contains the indices for the testing set, which needs to be excluded when training the model:
```{r}
# Train models:
models = lapply(names(folds),function(namex) rpart(Class~.,data=data[-folds[[namex]],]))
names(models) = names(folds)
```
Then we need to extract model predictions and collect labels for testing: 
```{r}
# Generate predictions:
preds = sapply(names(models),function(namex) predict(models[[namex]],data[folds[[namex]],],type = 'prob')[,2])

# Make sure response variable is numeric (binary):
y = as.numeric(factor(data[,'Class'])) - 1

# Collect labels for testing:
labels = sapply(folds,function(x) y[x])
```
Finally, we can make a cross-validation output, which gives a summary of AUC estimates as well as the range and average of ROC-curves:
```{r,fig.width=4,fig.height=4}
cv.out = cv.roc.curve(preds,labels,plot = TRUE)
```